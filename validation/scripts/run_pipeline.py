"""
run_pipeline.py
AI æ‰¹æ”¹ç³»ç»Ÿ - æ•°å­¦è¯•å· OCR éªŒè¯ Pipeline ä¸€é”®å…¥å£

ä½¿ç”¨æ–¹å¼ï¼š
  python scripts/run_pipeline.py [--step all|pdf|ocr|verify|report]

ç¯å¢ƒå˜é‡ï¼ˆè¿è¡Œå‰è®¾ç½®ï¼‰ï¼š
  $env:DASHSCOPE_API_KEY = "sk-xxxx"      # ä½¿ç”¨ Qwen2.5-VL
  # æˆ–
  $env:OPENAI_API_KEY    = "sk-xxxx"      # ä½¿ç”¨ GPT-4o
  $env:OCR_PROVIDER      = "openai"       # åˆ‡æ¢æä¾›æ–¹ï¼ˆé»˜è®¤ dashscopeï¼‰
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# ç¡®ä¿ scripts/ ç›®å½•åœ¨ sys.path
SCRIPTS_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(SCRIPTS_DIR))

from pdf_to_images import convert_pdfs
from ocr_extract import run_extraction
from math_verify import verify_ocr_results
from model_config import list_presets

PROJECT_ROOT = SCRIPTS_DIR.parent.parent
MATH_DIR = PROJECT_ROOT / "math"
OUTPUT_DIR = PROJECT_ROOT / "validation" / "output"
REPORT_DIR = PROJECT_ROOT / "validation" / "report"


def step_pdf_to_images():
    print("\n" + "=" * 50)
    print("STEP 1: PDF â†’ å›¾ç‰‡è½¬æ¢")
    print("=" * 50)
    images_dir = OUTPUT_DIR / "images"
    convert_pdfs(MATH_DIR, OUTPUT_DIR, dpi=300)
    return images_dir


def step_ocr_extract(mock: bool = False, preset: str | None = None):
    print("\n" + "=" * 50)
    print("STEP 2: VLM OCR é¢˜ç›®è¯†åˆ«" + (" [MOCK]" if mock else f" [{preset or 'default'}]"))
    print("=" * 50)
    images_dir = OUTPUT_DIR / "images"
    ocr_dir = OUTPUT_DIR / "ocr_results"

    if not images_dir.exists():
        print("[ERROR] è¯·å…ˆè¿è¡Œ step 1 (pdf)")
        sys.exit(1)

    return run_extraction(images_dir, ocr_dir, mock=mock, preset=preset)


def step_verify(no_model: bool = False):
    print("\n" + "=" * 50)
    suffix = " [çº¯SymPy]" if no_model else " [SymPy + æ¨¡å‹æ‰¹æ”¹]"
    print(f"STEP 3: SymPy æ•°å­¦éªŒè¯{suffix}")
    print("=" * 50)
    ocr_dir = OUTPUT_DIR / "ocr_results"
    verify_dir = OUTPUT_DIR / "verify_results"
    images_dir = OUTPUT_DIR / "images"
    verify_dir.mkdir(parents=True, exist_ok=True)

    if not ocr_dir.exists():
        print("[ERROR] è¯·å…ˆè¿è¡Œ step 2 (ocr)")
        sys.exit(1)

    results = []
    for json_file in sorted(ocr_dir.glob("*_ocr.json")):
        print(f"\n[INFO] éªŒè¯: {json_file.name}")
        r = verify_ocr_results(json_file, verify_dir, use_model=not no_model,
                               images_dir=images_dir if images_dir.exists() else None)
        results.append(r)
    return results


def step_report(verify_results: list[dict] | None = None):
    print("\n" + "=" * 50)
    print("STEP 4: ç”ŸæˆéªŒè¯æŠ¥å‘Š")
    print("=" * 50)
    REPORT_DIR.mkdir(parents=True, exist_ok=True)

    # è¯»å–æ‰€æœ‰éªŒè¯ç»“æœï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥ï¼‰
    if verify_results is None:
        verify_dir = OUTPUT_DIR / "verify_results"
        verify_results = []
        for jf in sorted(verify_dir.glob("*_verify.json")):
            with open(jf, encoding="utf-8") as f:
                verify_results.append(json.load(f))

    if not verify_results:
        print("[WARN] æ²¡æœ‰æ‰¾åˆ°éªŒè¯ç»“æœï¼Œè¯·å…ˆè¿è¡Œ verify æ­¥éª¤")
        return

    # ç»Ÿè®¡æ±‡æ€»
    total_q = sum(r["total_questions"] for r in verify_results)
    auto_verified = sum(r["auto_verified"] for r in verify_results)
    correct = sum(r.get("correct", 0) for r in verify_results)
    incorrect = sum(r.get("incorrect", 0) for r in verify_results)
    needs_manual = sum(r["needs_manual_review"] for r in verify_results)

    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    ocr_model = verify_results[0].get("ocr_model", "unknown") if verify_results else "unknown"
    grade_preset = verify_results[0].get("grade_model_preset", "") if verify_results else ""

    lines = [
        f"# æ•°å­¦è¯•å· OCR éªŒè¯æŠ¥å‘Š",
        f"",
        f"> ç”Ÿæˆæ—¶é—´ï¼š{now}  ",
        f"> OCR æ¨¡å‹ï¼š`{ocr_model}`  ",
        f"> æ‰¹æ”¹æ¨¡å‹ï¼š`{grade_preset}`  ",
        f"> è¾“å…¥è¯•å·ç›®å½•ï¼š`math/`",
        f"",
        f"---",
        f"",
        f"## ğŸ“Š æ€»ä½“æ±‡æ€»",
        f"",
        f"| æŒ‡æ ‡ | æ•°å€¼ |",
        f"|------|------|",
        f"| å¤„ç†è¯•å·æ•° | {len(verify_results)} ä»½ |",
        f"| è¯†åˆ«é¢˜ç›®æ€»æ•° | {total_q} é¢˜ |",
        f"| âœ… æ­£ç¡® | {correct} é¢˜ |",
        f"| âŒ é”™è¯¯ | {incorrect} é¢˜ |",
        f"| ğŸ” å¾…å¤æ ¸ | {needs_manual} é¢˜ |",
        f"| è‡ªåŠ¨åˆ¤å®šç‡ | {auto_verified/total_q*100:.1f}% |" if total_q > 0 else "| è‡ªåŠ¨åˆ¤å®šç‡ | N/A |",
        f"",
        f"---",
        f"",
    ]

    for paper in verify_results:
        src = paper["source_file"]
        lines += [
            f"## ğŸ“„ {src}",
            f"",
            f"- é¢˜ç›®æ•°ï¼š{paper['total_questions']}  |  "
            f"âœ…{paper.get('correct',0)}  âŒ{paper.get('incorrect',0)}  "
            f"ğŸ”{paper['needs_manual_review']}",
            f"",
            f"| é¢˜å· | ç±»å‹ | é¢˜ç›® | å­¦ç”Ÿç­”æ¡ˆ | å‚è€ƒç­”æ¡ˆ(æ¥æº) | æ­£ç¡®æ€§ |",
            f"|------|------|------|---------|---------------|--------|",
        ]
        for r in paper.get("results", []):
            is_correct = (
                "âœ…" if r.get("is_correct") is True
                else ("âŒ" if r.get("is_correct") is False else "ğŸ”")
            )
            q_text = (r.get("question_text") or "")[:40]
            s_ans = (r.get("student_answer") or "")[:25]
            # æ‰¾æœ€é«˜ç½®ä¿¡ç­”æ¡ˆ
            answers = r.get("answers", [])
            if answers:
                best = answers[0]
                ref = f"`{(best.get('value','') or '')[:20]}` ({best.get('source','')})"
            else:
                ref = "-"
            lines.append(
                f"| {r['question_id']} | {r['answer_area_type']} | "
                f"{q_text} | `{s_ans}` | {ref} | {is_correct} |"
            )
        lines.append("")

    lines += [
        "---",
        "",
        "## ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®",
        "",
        "1. å¯¹ã€ŒğŸ”ã€çš„é¢˜ç›®è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œè¡¥å……æ ‡å‡†ç­”æ¡ˆ",
        "2. ç»æ ‡æ³¨åé‡æ–°è¿è¡Œ `math_verify.py` ä»¥è·å¾—å‡†ç¡®ç‡æ•°æ®",
        "3. æ ¹æ®é”™è¯¯æ¨¡å¼åˆ†æï¼Œè°ƒæ•´ Prompt æˆ–å¾®è°ƒæ¨¡å‹",
        "",
        "*æœ¬æŠ¥å‘Šç”± AI æ‰¹æ”¹ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼ŒéªŒè¯ç»“æœä¾›å‚è€ƒã€‚*",
    ]

    report_path = REPORT_DIR / "validation_report.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"   æ€»è®¡ {total_q} é“é¢˜ | è‡ªåŠ¨éªŒè¯ {auto_verified} é“ | å¾…å¤æ ¸ {needs_manual} é“")


def main():
    parser = argparse.ArgumentParser(
        description="AI æ‰¹æ”¹ç³»ç»Ÿ - æ•°å­¦ OCR éªŒè¯ Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--step",
        choices=["all", "pdf", "ocr", "verify", "report"],
        default="all",
        help="æ‰§è¡ŒæŒ‡å®šæ­¥éª¤ï¼ˆé»˜è®¤ allï¼‰",
    )
    parser.add_argument(
        "--mock",
        action="store_true",
        help="OCR ä½¿ç”¨å ä½æ•°æ®ï¼Œæ— éœ€æ¨¡å‹ï¼ˆéªŒè¯æµç¨‹ç”¨ï¼‰",
    )
    parser.add_argument(
        "--preset",
        default=None,
        help="æ¨¡å‹ presetï¼ˆollama-qwen-vl / ollama-deepseek-ocr / ollama-gpt-oss / dashscope / openaiï¼‰",
    )
    parser.add_argument(
        "--list-presets",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹é¢„è®¾",
    )
    parser.add_argument(
        "--no-model",
        action="store_true",
        help="éªŒè¯æ­¥éª¤è·³è¿‡æ¨¡å‹æ‰¹æ”¹ï¼ˆçº¯ SymPy + å­—ç¬¦ä¸²åŒ¹é…ï¼‰",
    )
    args = parser.parse_args()

    if args.list_presets:
        print(list_presets())
        sys.exit(0)

    print("\nğŸš€ AI æ‰¹æ”¹ç³»ç»Ÿ - OCR éªŒè¯ Pipeline å¯åŠ¨")
    print(f"   é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"   æ•°å­¦æ ·å·ç›®å½•: {MATH_DIR}")
    print(f"   è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"   æ¨¡å¼: {'MOCK' if args.mock else f'REAL [{args.preset or "AI_MODEL_PRESET env"}]'}")
    if not args.no_model:
        print(f"   æ‰¹æ”¹æ¨¡å‹: GRADE_MODEL_PRESET env æˆ–é»˜è®¤")
    print()

    if args.step in ("all", "pdf"):
        step_pdf_to_images()

    if args.step in ("all", "ocr"):
        step_ocr_extract(mock=args.mock, preset=args.preset)

    verify_results = None
    if args.step in ("all", "verify"):
        verify_results = step_verify(no_model=args.no_model)

    if args.step in ("all", "report"):
        step_report(verify_results)

    print("\nğŸ‰ Pipeline å®Œæˆï¼")


if __name__ == "__main__":
    main()
